{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2, itertools\n",
    "from imutils import build_montages as bm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report as cr, confusion_matrix as cfm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Dense, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.utils import np_utils as nu\n",
    "from keras.utils.np_utils import to_categorical as tc\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "def split_train_data(train_data, train_label):\n",
    "\t# Split into 48,000 train data and 12,000 validation data\n",
    "\tpartial_train_data = train_data[:48000]\n",
    "\tpartial_train_label = train_label[:48000]\n",
    "\tvalidation_data = train_data[48000:]\n",
    "\tvalidation_label = train_label[48000:]\n",
    "\treturn partial_train_data, partial_train_label, validation_data, validation_label\n",
    "\n",
    "def data_exploration(train, val, test):\n",
    "\tprint(\"\\nFASHION-MNIST DATASET EXPLORATION\")\n",
    "\tprint(\"Size of Train Data: \", train.shape)\n",
    "\tprint(\"Size of Validation Data: \", val.shape)\n",
    "\tprint(\"Size of Test Data: \", test.shape)\n",
    "\tprint()\n",
    "\n",
    "def convert_into_4D(partial_train_data, validation_data, test_data):\n",
    "\t# If we are using \"channels first\" ordering, then reshape the design\n",
    "\t# matrix such that the matrix is: num_samples x depth x rows x columns\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tpartial_train_data = train_data.reshape((partial_train_data.shape[0], 1, 28, 28))\n",
    "\t\tvalidation_data = validation_data.reshape((validation_data.shape[0], 1, 28, 28))\n",
    "\t\ttest_data = test_data.reshape((test_data.shape[0], 1, 28, 28))\n",
    "\t\t\n",
    "\t# otherwise, we are using \"channels last\" ordering, so the design\n",
    "\t# matrix shape should be: num_samples x rows x columns x depth\n",
    "\telse:\n",
    "\t\tpartial_train_data = partial_train_data.reshape((partial_train_data.shape[0], 28, 28, 1))\n",
    "\t\tvalidation_data = validation_data.reshape((validation_data.shape[0], 28, 28, 1))\n",
    "\t\ttest_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n",
    "\t\n",
    "\treturn partial_train_data, validation_data, test_data\n",
    "\n",
    "def optimize(cnn_model):\n",
    "\t# Adam Optimizer and Cross Entropy Loss\n",
    "\tcnn_model.compile(optimizer=Nadam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\tprint(cnn_model.summary())\n",
    "\n",
    "\t# Use TensorBoard\n",
    "\t# Step 1. In terminal, script \"tensorboard --logdir ./Graph\" (without \"\")\n",
    "\t# Step 2. In Chrome, type url localhost:6006 (or any IP emerged when Step 1 applied)\n",
    "\n",
    "\t# Train for 100 Epochs and use TensorBoard Callback\n",
    "\tresult = cnn_model.fit(\n",
    "\t\tpartial_train_data, partial_train_label,\n",
    "\t\tbatch_size=number_of_batch,\n",
    "\t\tepochs=number_of_epoch, verbose=1, # verbose = 0 (silent) / verbose = 1 (progress bar)\n",
    "\t\tvalidation_data=(validation_data, validation_label),\n",
    "\t\tcallbacks=[TensorBoard(log_dir='./Graph')])\n",
    "\n",
    "\t# Save Weights\n",
    "\tcnn_model.save_weights('{}.ckpt'.format(cnn_model.name))\n",
    "\treturn cnn_model, result\n",
    "\n",
    "def cnn_model_one():\n",
    "\tcnn_model_label = \"ConvNet Model 1\"\n",
    "\tinputs = Input(shape=(28, 28, 1))\n",
    "\tconv_layer = ZeroPadding2D(padding=(2,2))(inputs)\n",
    "\tconv_layer = Conv2D(16, (5, 5), strides=(1,1), activation='relu')(conv_layer)\n",
    "\tconv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "\tconv_layer = Conv2D(32, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\tconv_layer = Conv2D(32, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\tconv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "\tconv_layer = Conv2D(64, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\tflatten = Flatten()(conv_layer) # Flatten feature map to Vector with 576 element\n",
    "\tfc_layer = Dense(256, activation='relu')(flatten) # Fully-connected layer\n",
    "\tfc_layer = Dense(64, activation='relu')(fc_layer) \n",
    "\toutputs = Dense(10, activation='softmax')(fc_layer)\n",
    "\n",
    "\tmodel_1 = Model(name=cnn_model_label, inputs=inputs, outputs=outputs)\n",
    "\treturn model_1\n",
    "\n",
    "def cnn_model_two():\n",
    "\tcnn_model_label = \"ConvNet Model 2\"\n",
    "\tinputs = Input(shape=(28, 28, 1))\n",
    "\tconv_layer = Conv2D(16, (14, 14), strides=(1,1), activation='relu')(inputs)\n",
    "\tconv_layer = Conv2D(16, (7, 7), strides=(1,1), activation='relu')(inputs)\n",
    "\tconv_layer = Conv2D(16, (5, 5), strides=(1,1), activation='relu')(inputs)\n",
    "\tconv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "\tconv_layer = Conv2D(32, (3, 3), strides=(1,1), activation='tanh')(conv_layer)\n",
    "\tconv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "\tconv_layer = Conv2D(32, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\tconv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "\tflatten = Flatten()(conv_layer)\n",
    "\tfc_layer = Dense(256, activation='relu')(flatten)\n",
    "\tfc_layer = Dense(64, activation='tanh')(fc_layer) \n",
    "\toutputs = Dense(10, activation='softmax')(flatten)\n",
    "\t\n",
    "\tmodel_2 = Model(name=cnn_model_label, inputs=inputs, outputs=outputs)\n",
    "\treturn model_2\n",
    "\n",
    "def training_and_validation_loss(loss):\n",
    "\tplt.figure(\"Training & Val Loss\")\n",
    "\tplt.plot(loss.history['loss'])\n",
    "\tplt.plot(loss.history['val_loss'])\n",
    "\tplt.title(\"Model Loss\")\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.legend(['Train', 'Test'])\n",
    "\tplt.show()\n",
    "\n",
    "def training_and_validation_accuracy(acc):\n",
    "\tplt.figure(\"Training & Val Accuracy\")\n",
    "\tplt.plot(acc.history['accuracy'])\n",
    "\tplt.plot(acc.history['val_accuracy'])\n",
    "\tplt.title(\"Model Accuracy\")\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.legend(['Train', 'Test'])\n",
    "\tplt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\tplt.figure(\"Confusion Matrix\")\n",
    "\tplt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\tplt.title(title)\n",
    "\tplt.colorbar()\n",
    "\tlegend = np.arange(len(classes))\n",
    "\tplt.xticks(legend, classes, rotation=90)\n",
    "\tplt.yticks(legend, classes)\n",
    "\t\n",
    "\tif normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\t\n",
    "\tthresh = cm.max() / 2.\n",
    "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\t\tplt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\tplt.ylabel('ACTUAL')\n",
    "\tplt.xlabel('PREDICTIVE')\n",
    "\tplt.show()\n",
    "\n",
    "def random_test_images(a):\n",
    "\timages = []\n",
    "\tfor i in np.random.choice(np.arange(0, len(test_label)), size=(16)):\n",
    "\t\tprobs = a.predict(test_data[np.newaxis, i])\n",
    "\t\tprediction = probs.argmax(axis=1)\n",
    "\t\tlabel = label_names[prediction[0]]\n",
    "\n",
    "\t\tif K.image_data_format() == \"channels_first\": image = (test_data[i][0] * 255).astype(\"uint8\")\n",
    "\t\telse: image = (test_data[i] * 255).astype(\"uint8\")\n",
    "\n",
    "\t\tcolor = (0, 255, 0) # Label color default = green (correct)\n",
    "\t\tif prediction[0] != np.argmax(test_label[i]): color = (0, 0, 255)  # Label color = red (incorrect)\n",
    "\n",
    "\t\timage = cv2.merge([image] * 3) # merge the channels into one image\n",
    "\t\timage = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR) # resize from 28x28 to 96x96\n",
    "\t\tcv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 2)\n",
    "\t\timages.append(image)\n",
    "\n",
    "\tcv2.imshow(\"Fashion MNIST - Random Image Test\", bm(images, (96, 96), (4, 4))[0])\n",
    "\tcv2.waitKey(0)\n",
    "\n",
    "############################### INITIALIZATION ###############################\n",
    "learning_rate = 1e-3\n",
    "number_of_epoch = 100\n",
    "number_of_batch = 256\n",
    "label_names = [\"Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "############################## DATA PREPARATION ##############################\n",
    "# NOTES: Fashion MNIST is already organized into 60,000 training/ 10,000 testing splits\n",
    "(train_data, train_label), (test_data, test_label) = fashion_mnist.load_data()\n",
    "\n",
    "# Scale/normalize data to the range of [0, 1]\n",
    "train_data = train_data.astype(\"float32\") / 255.0\n",
    "test_data = test_data.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode the training and testing labels\n",
    "train_label = tc(train_label, 10)\n",
    "test_label = tc(test_label, 10)\n",
    "\n",
    "partial_train_data, partial_train_label, validation_data, validation_label = split_train_data(train_data, train_label)\n",
    "data_exploration(partial_train_data, validation_data, test_data)\n",
    "partial_train_data, validation_data, test_data = convert_into_4D(partial_train_data, validation_data, test_data)\n",
    "\n",
    "############################### SELECT CNN MODEL ##############################\n",
    "select_model = 0\n",
    "while select_model < 1 or select_model > 2:\n",
    "\tselect_model = int(input(\"Select CNN Model [1-2]: \"))\n",
    "\n",
    "if select_model == 1:\n",
    "\tmodel_1, result_1 = optimize(cnn_model_one()) # Train CNN Model 1\n",
    "\n",
    "\tprediction_1 = model_1.predict(test_data)\n",
    "\tprint(\"Evaluate Test\")\n",
    "\tmodel_1.evaluate(test_data, test_label)\n",
    "\tprint(cr(test_label.argmax(axis=1), prediction_1.argmax(axis=1), target_names=label_names)) # Classification Report\n",
    "\n",
    "\t# Training and Validation Curves\n",
    "\ttraining_and_validation_accuracy(result_1)\n",
    "\ttraining_and_validation_loss(result_1)\n",
    "\n",
    "\t# Confusion Matrix Visualization\n",
    "\tprediction_class_1 = np.argmax(prediction_1, axis=1) # Convert predictions classes to one hot vectors \n",
    "\ttest_label_cfm = np.argmax(test_label, axis=1) # Convert validation observations to one hot vectors\n",
    "\tconfusion_mtx = cfm(test_label_cfm, prediction_class_1) # Compute the confusion matrix\n",
    "\tplot_confusion_matrix(confusion_mtx, classes=label_names) # Plot the confusion matrix\n",
    "\n",
    "\trandom_test_images(model_1)\n",
    "else:\n",
    "\tmodel_2, result_2 = optimize(cnn_model_two()) # Train CNN Model 2\n",
    "\n",
    "\tprediction_2 = model_2.predict(test_data)\n",
    "\tprint(\"Evaluate Test\")\n",
    "\tmodel_2.evaluate(test_data, test_label)\n",
    "\tprint(cr(test_label.argmax(axis=1), prediction_2.argmax(axis=1), target_names=label_names)) # Classification Report\n",
    "\n",
    "\t# Training and Validation Curves\n",
    "\ttraining_and_validation_accuracy(result_2)\n",
    "\ttraining_and_validation_loss(result_2)\n",
    "\n",
    "\t# Confusion Matrix Visualization\n",
    "\tprediction_class_2 = np.argmax(prediction_2, axis=1) # Convert predictions classes to one hot vectors \n",
    "\ttest_label_cfm = np.argmax(test_label, axis=1) # Convert validation observations to one hot vectors\n",
    "\tconfusion_mtx = cfm(test_label_cfm, prediction_class_2) # Compute the confusion matrix\n",
    "\tplot_confusion_matrix(confusion_mtx, classes=label_names) # Plot the confusion matrix\n",
    "\n",
    "\trandom_test_images(model_2)\n",
    "\n",
    "# REFERENCES\n",
    "# https://www.pyimagesearch.com/2019/02/11/fashion-mnist-with-keras-and-deep-learning/\n",
    "# https://www.kaggle.com/fuzzywizard/fashion-mnist-cnn-keras-accuracy-93\n",
    "# https://medium.com/@samuelsena/pengenalan-deep-learning-part-7-convolutional-neural-network-cnn-b003b477dc94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
